{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7oNgtQf2r-I"
      },
      "outputs": [],
      "source": [
        "from music21 import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function to read MIDI files\n",
        "def read_midi(file):\n",
        "\n",
        "    print(\"Loading Music File:\",file)\n",
        "\n",
        "    notes=[]\n",
        "    notes_to_parse = None\n",
        "\n",
        "    #parsing a midi file\n",
        "    midi = converter.parse(file)\n",
        "\n",
        "    #grouping based on different instruments\n",
        "    s2 = instrument.partitionByInstrument(midi)\n",
        "\n",
        "    #Looping over all the instruments\n",
        "    for part in s2.parts:\n",
        "\n",
        "        #select elements of only piano\n",
        "        if 'Piano' in str(part):\n",
        "\n",
        "            notes_to_parse = part.recurse()\n",
        "\n",
        "            #finding whether a particular element is note or a chord\n",
        "            for element in notes_to_parse:\n",
        "\n",
        "                #note\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "\n",
        "                #chord\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    return np.array(notes)"
      ],
      "metadata": {
        "id": "ahgs2qPH3BCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for listing down the file names\n",
        "import os\n",
        "\n",
        "#Array Processing\n",
        "import numpy as np\n",
        "\n",
        "#specify the path\n",
        "path='/content/midi_files/'\n",
        "\n",
        "#read all the filenames\n",
        "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "\n",
        "all_notes = []\n",
        "\n",
        "# Reading and processing each MIDI file\n",
        "for file in files:\n",
        "    midi_notes = read_midi(path + file)\n",
        "    all_notes.extend(midi_notes)\n",
        "\n",
        "# Convert the list of notes/chords to a NumPy array\n",
        "notes_array = np.array(all_notes)\n",
        "print(notes_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_K4zCTA3Ini",
        "outputId": "0297d5a0-05d7-451e-96fa-dada822bffe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Music File: /content/midi_files/youre only lonely L.mid\n",
            "Loading Music File: /content/midi_files/Pirates of the Caribbean .mid\n",
            "Loading Music File: /content/midi_files/Never-Gonna-Give-You-Up-3.mid\n",
            "['D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4'\n",
            " 'D4' 'D4' 'D4' 'D4' 'D4' '2' 'D4' 'D4' 'D4' 'D4' '2' 'D4' 'D4' '2' 'A3'\n",
            " 'C4' '2.5.9' '2' '2.5.9' '2' '2' '2.5.9' '9.0.4' '0' '10.2.5' '10'\n",
            " '10.2.5' '10' '10' '10.2.5' '2.7' '10' '9.0.4' '9' '9.0.4' '9' '9' '9.2'\n",
            " '7.0' '9' '9.0' '2' '9.2' '2' '2' 'A3' 'C4' '2' '10.2.5' '10' '10.2.5'\n",
            " '10' '10' '10.2' '4.10' '10' '5.9.0' '10' '5.9.0' '10' '10' '0.5' '7.0'\n",
            " '10' '9.0.4' '9' '9.0.4' '9' '9' '9.2' '7.0' '9' '2.5.9' '2' '2' '2' 'A3'\n",
            " 'C4' '2' '2.5.9' '2' '2.5.9' '2' '2' '9.2' '5.9' '2' '7.10.2' '10'\n",
            " '7.10.2' '10' '10' '2.7' '9.2' '10' '7.10.2' '7' '7.10.2' '7' '7' '5.9'\n",
            " '4.7' '7' '5.9' '2' 'D4' '2' '2' 'D4' 'E4' '2' '10.2.5' '10' '10.2.5'\n",
            " '10' '10' '7.10.2' '10' '5.9' '2' 'D4' '2' '2' 'D4' 'F4' '2' '9.1.4' '9'\n",
            " '9.1.4' '9' '9' '2.5' '11.2' '9' '9.1.4' '9' '9' '9' 'A4' 'C5' '9'\n",
            " '2.5.9' '2' '2.5.9' '2' '2' '2.5.9' '9.0.4' '0' '10.2.5' '10' '10.2.5'\n",
            " '10' '10' '10.2.5' '2.7' '10' '9.0.4' '9' '9.0.4' '9' '9' '9.2' '7.0' '9'\n",
            " '9.0' '2' '9.2' '2' '2' 'A4' 'C5' '2' '10.2.5' '10' '10.2.5' '10' '10'\n",
            " '10.2' '4.10' '10' '5.9.0' '5' '5.9.0' '5' '5' '0.5' '7.0' '5' '0.4' '0'\n",
            " '0.4' '0' '9' '9.2' 'C5' '9' '2.5.9' '2' '2' '2' 'A4' 'C5' '2' '2.5.9'\n",
            " '2' '2.5.9' '2' '2' '9.2' '5.9' '2' '7.10.2' '10' '7.10.2' '10' '10'\n",
            " '2.7' '9.2' '10' '7.10.2' '7' '7.10.2' '7' '7' '5.9' '4.7' '7' '5.9' '2'\n",
            " 'D5' '2' '2' 'D5' 'E5' '2' '10.2.5' '10' '10.2.5' '10' '10' '7.10.2' '10'\n",
            " '5.9' '2' 'D5' '2' '2' 'D5' 'F5' '2' '9.1.4' '9' '9.1.4' '9' '9' 'D5'\n",
            " 'C#5' '9' '9.2' '2' '9.2' '2' '9.0.4' '0' '0.2.5' '0' 'F5' '0' 'F5'\n",
            " '7.10.2' '10' '9.2' '9' 'F5' '9' '9' '5.9' '9.2' '9' 'A4' '9' '9' '9' '9'\n",
            " '7.10.2' '7' '7' '7' '7.10' '10.2' '7' 'B-4' '7' '7' '7' '7' '1.4' '9'\n",
            " '1.4' '9' '2.7' '9' '9' '1.5.9' '9' '9' '9' 'F4' 'G4' '9' '2.5.9' '2'\n",
            " '2.5.9' '2' '2' '2.5.9' '2' '10.2.5' '2' '2.5.9' '2' '2' '2' '2' '0.4.7'\n",
            " '0' '0.4.7' '0' '0' '0.4.7' '0' '0.4.7' '5' '5.9.0' '5' '5' '5' '5'\n",
            " '2.5.9' '2' '2.5.9' '2' '2' '2.5.9' '2' '10.2.5' '2' '2.5.9' '2' '2' '2'\n",
            " '2' '1.4.7' '9' '1.5' '9' '9' '4.9' '9' '2.5.9' '2' '2' '2' 'D4' 'E4' '2'\n",
            " '2.5.9' '2' '2' '2' 'G4' 'A4' '2' '7.0' '0' '0.5' '0' '0' '0.4' '0' '0'\n",
            " '5.9.0' '5' '7.9.0' '5' '5' '9.0' '5' '5' '0.4.7' '0' '0' '0' 'F4' '0'\n",
            " 'G4' '0' '5.9.0' '5' '5' '5' 'G4' '5' 'F4' '5' '1.4' '9' '1.5' '9' '9'\n",
            " '1.4' '9' '9' '2.5.9' '2' '2' '2' 'E4' 'C4' '2' '2.5.9' '2' '2' '2' 'D5'\n",
            " '2' 'E5' '2' '2.5.9' '2' '2' '2' 'E5' '2' 'F5' '2' '7.0' '0' '0.5' '0'\n",
            " '0' '7.0' '0' '0' '5.9' '5' '7.0' '5' '5' '0.5' '5' '5' '10.2.5' '10'\n",
            " '10' '10' 'D5' '10' 'E5' '10' '2.5.9' '2' '7.9.2' '2' '2' '9.2' '2' '2'\n",
            " '10.2' '7' '10.2' '7' '7' '7.10' '7' '7' '5.9' '9' '9' '9' 'G5' '9' 'E5'\n",
            " '9' '9.2' '9' '9' '9' 'E5' '9' 'C#5' '9' '2.5.9' '2' '2' '2' '2' '2'\n",
            " '7.10.2' '7' '7' '7' '7' '7' '5.9.0' '5' '5.9.0' '5' '5' '5.9.0' '5'\n",
            " '9.0.4' '0' 'G5' '0' '0' '0' '0' '7.10.2' '7' '7' '7' '7' '7' '2.5.9' '9'\n",
            " '9' '9' '9' '9' '5.9' '9' '7.9' '9' '9' '4.9' '9' '9' '2.5.9' '2' '2'\n",
            " 'D5' '2' 'E5' '2' 'F5' '2' '2.5.9' '2' '2' 'D5' '2' 'E5' '2' 'F5' '2'\n",
            " '10.2.5' '10' '10' 'D5' '10' 'E5' '10' 'F5' '10' '5.9.0' '5' '5.9.0' '5'\n",
            " '5' '0.5' '5' '5' '9.0.4' '0' 'G5' '0' '0' '0' '0' '7.10.2' '7' '7' '7'\n",
            " '7' '7' '2.5.9' '9' '9' '9' '9' '9' '5.9' '9' '7.9' '9' '9' '4.9' '9' '9'\n",
            " '2.5.9' '2' '2' '2' '2' 'D4' '2' 'D4' '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting 2D array into 1D array\n",
        "notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "#No. of unique notes\n",
        "unique_notes = list(set(notes_))\n",
        "print(len(unique_notes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lceH8h3m60Rt",
        "outputId": "bec4295c-1e44-455c-8884-98721441c7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library\n",
        "from collections import Counter\n",
        "\n",
        "#computing frequency of each note\n",
        "freq = dict(Counter(notes_))\n",
        "\n",
        "#library for visualiation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#consider only the frequencies\n",
        "no=[count for _,count in freq.items()]\n",
        "\n",
        "#set the figure size\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "#plot\n",
        "plt.hist(no)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "_kw2L9UC66yb",
        "outputId": "173eea13-b687-4140-bde9-57ed1416d1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([9., 1., 1., 2., 0., 1., 2., 1., 0., 1.]),\n",
              " array([  1. ,  29.4,  57.8,  86.2, 114.6, 143. , 171.4, 199.8, 228.2,\n",
              "        256.6, 285. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 426,
              "height": 428
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "print(len(frequent_notes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unyYz3R77R0T",
        "outputId": "df6de00c-8230-40e9-9fbf-2e1c327d7bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example of frequent_notes\n",
        "frequent_notes = [8]  # Sample list of frequent notes\n",
        "\n",
        "# Assuming notes_array is a list of lists with different lengths\n",
        "notes_array = ['D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4' 'D4'\n",
        " 'D4' 'D4' 'D4' 'D4' 'D4' '2' 'D4' 'D4' 'D4' 'D4' '2' 'D4' 'D4' '2' 'A3'\n",
        " 'C4' '2.5.9' '2' '2.5.9' '2' '2' '2.5.9' '9.0.4' '0' '10.2.5' '10'\n",
        " '10.2.5' '10' '10' '10.2.5' '2.7' '10' '9.0.4' '9' '9.0.4' '9' '9' '9.2'\n",
        " '7.0' '9' '9.0' '2' '9.2' '2' '2' 'A3' 'C4' '2' '10.2.5' '10' '10.2.5'\n",
        " '10' '10' '10.2' '4.10' '10' '5.9.0' '10' '5.9.0' '10' '10' '0.5' '7.0'\n",
        " '10' '9.0.4' '9' '9.0.4' '9' '9' '9.2' '7.0' '9' '2.5.9' '2' '2' '2' 'A3'\n",
        " 'C4' '2' '2.5.9' '2' '2.5.9' '2' '2' '9.2' '5.9' '2' '7.10.2' '10'\n",
        " '7.10.2' '10' '10' '2.7' '9.2' '10' '7.10.2' '7' '7.10.2' '7' '7' '5.9'\n",
        " '4.7' '7' '5.9' '2' 'D4' '2' '2' 'D4' 'E4' '2' '10.2.5' '10' '10.2.5'\n",
        " '10' '10' '7.10.2' '10' '5.9' '2' 'D4' '2' '2' 'D4' 'F4' '2' '9.1.4' '9'\n",
        " '9.1.4' '9' '9' '2.5' '11.2' '9' '9.1.4' '9' '9' '9' 'A4' 'C5' '9'\n",
        " '2.5.9' '2' '2.5.9' '2' '2' '2.5.9' '9.0.4' '0' '10.2.5' '10' '10.2.5'\n",
        " '10' '10' '10.2.5' '2.7' '10' '9.0.4' '9' '9.0.4' '9' '9' '9.2' '7.0' '9'\n",
        " '9.0' '2' '9.2' '2' '2' 'A4' 'C5' '2' '10.2.5' '10' '10.2.5' '10' '10'\n",
        " '10.2' '4.10' '10' '5.9.0' '5' '5.9.0' '5' '5' '0.5' '7.0' '5' '0.4' '0'\n",
        " '0.4' '0' '9' '9.2' 'C5' '9' '2.5.9' '2' '2' '2' 'A4' 'C5' '2' '2.5.9'\n",
        " '2' '2.5.9' '2' '2' '9.2' '5.9' '2' '7.10.2' '10' '7.10.2' '10' '10'\n",
        " '2.7' '9.2' '10' '7.10.2' '7' '7.10.2' '7' '7' '5.9' '4.7' '7' '5.9' '2'\n",
        " 'D5' '2' '2' 'D5' 'E5' '2' '10.2.5' '10' '10.2.5' '10' '10' '7.10.2' '10'\n",
        " '5.9' '2' 'D5' '2' '2' 'D5' 'F5' '2' '9.1.4' '9' '9.1.4' '9' '9' 'D5'\n",
        " 'C#5' '9' '9.2' '2' '9.2' '2' '9.0.4' '0' '0.2.5' '0' 'F5' '0' 'F5'\n",
        " '7.10.2' '10' '9.2' '9' 'F5' '9' '9' '5.9' '9.2' '9' 'A4' '9' '9' '9' '9'\n",
        " '7.10.2' '7' '7' '7' '7.10' '10.2' '7' 'B-4' '7' '7' '7' '7' '1.4' '9'\n",
        " '1.4' '9' '2.7' '9' '9' '1.5.9' '9' '9' '9' 'F4' 'G4' '9' '2.5.9' '2'\n",
        " '2.5.9' '2' '2' '2.5.9' '2' '10.2.5' '2' '2.5.9' '2' '2' '2' '2' '0.4.7'\n",
        " '0' '0.4.7' '0' '0' '0.4.7' '0' '0.4.7' '5' '5.9.0' '5' '5' '5' '5'\n",
        " '2.5.9' '2' '2.5.9' '2' '2' '2.5.9' '2' '10.2.5' '2' '2.5.9' '2' '2' '2'\n",
        " '2' '1.4.7' '9' '1.5' '9' '9' '4.9' '9' '2.5.9' '2' '2' '2' 'D4' 'E4' '2'\n",
        " '2.5.9' '2' '2' '2' 'G4' 'A4' '2' '7.0' '0' '0.5' '0' '0' '0.4' '0' '0'\n",
        " '5.9.0' '5' '7.9.0' '5' '5' '9.0' '5' '5' '0.4.7' '0' '0' '0' 'F4' '0'\n",
        " 'G4' '0' '5.9.0' '5' '5' '5' 'G4' '5' 'F4' '5' '1.4' '9' '1.5' '9' '9'\n",
        " '1.4' '9' '9' '2.5.9' '2' '2' '2' 'E4' 'C4' '2' '2.5.9' '2' '2' '2' 'D5'\n",
        " '2' 'E5' '2' '2.5.9' '2' '2' '2' 'E5' '2' 'F5' '2' '7.0' '0' '0.5' '0'\n",
        " '0' '7.0' '0' '0' '5.9' '5' '7.0' '5' '5' '0.5' '5' '5' '10.2.5' '10'\n",
        " '10' '10' 'D5' '10' 'E5' '10' '2.5.9' '2' '7.9.2' '2' '2' '9.2' '2' '2'\n",
        " '10.2' '7' '10.2' '7' '7' '7.10' '7' '7' '5.9' '9' '9' '9' 'G5' '9' 'E5'\n",
        " '9' '9.2' '9' '9' '9' 'E5' '9' 'C#5' '9' '2.5.9' '2' '2' '2' '2' '2'\n",
        " '7.10.2' '7' '7' '7' '7' '7' '5.9.0' '5' '5.9.0' '5' '5' '5.9.0' '5'\n",
        " '9.0.4' '0' 'G5' '0' '0' '0' '0' '7.10.2' '7' '7' '7' '7' '7' '2.5.9' '9'\n",
        " '9' '9' '9' '9' '5.9' '9' '7.9' '9' '9' '4.9' '9' '9' '2.5.9' '2' '2'\n",
        " 'D5' '2' 'E5' '2' 'F5' '2' '2.5.9' '2' '2' 'D5' '2' 'E5' '2' 'F5' '2'\n",
        " '10.2.5' '10' '10' 'D5' '10' 'E5' '10' 'F5' '10' '5.9.0' '5' '5.9.0' '5'\n",
        " '5' '0.5' '5' '5' '9.0.4' '0' 'G5' '0' '0' '0' '0' '7.10.2' '7' '7' '7'\n",
        " '7' '7' '2.5.9' '9' '9' '9' '9' '9' '5.9' '9' '7.9' '9' '9' '4.9' '9' '9'\n",
        " '2.5.9' '2' '2' '2' '2' 'D4' '2' 'D4' '2']\n",
        "\n",
        "# Finding the maximum length of the lists\n",
        "max_len = max(len(notes) for notes in notes_array)\n",
        "\n",
        "# Padding the lists to have the same length\n",
        "new_music = []\n",
        "for notes in notes_array:\n",
        "    temp = []\n",
        "    for note_ in notes:\n",
        "        if note_ in frequent_notes:\n",
        "            temp.append(note_)\n",
        "    # Padding with zeros if needed\n",
        "    while len(temp) < max_len:\n",
        "        temp.append(0)\n",
        "    new_music.append(temp)\n",
        "\n",
        "# Convert the list of lists to a NumPy array\n",
        "new_music = np.array(new_music)\n",
        "print(new_music)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y3OIn7U69Rr",
        "outputId": "bf3f56cb-aeba-42f4-c0a0-4774fd6717a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_timesteps = 32\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for note_ in new_music:\n",
        "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
        "\n",
        "        #preparing input and output sequences\n",
        "        input_ = note_[i:i + no_of_timesteps]\n",
        "        output = note_[i + no_of_timesteps]\n",
        "\n",
        "        x.append(input_)\n",
        "        y.append(output)\n",
        "\n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "AjnrFHuJ89zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_x = list(set(x.ravel()))\n",
        "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
      ],
      "metadata": {
        "id": "u-ijLV6U9Agj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing input sequences\n",
        "x_seq=[]\n",
        "for i in x:\n",
        "    temp=[]\n",
        "    for j in i:\n",
        "        #assigning unique integer to every note\n",
        "        temp.append(x_note_to_int[j])\n",
        "    x_seq.append(temp)\n",
        "\n",
        "x_seq = np.array(x_seq)"
      ],
      "metadata": {
        "id": "7cS6vXFG9Gy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_y = list(set(y))\n",
        "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y))\n",
        "y_seq=np.array([y_note_to_int[i] for i in y])"
      ],
      "metadata": {
        "id": "JAAdjV-f9L7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "OiYCunxU9T4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm():\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128,return_sequences=True))\n",
        "  model.add(LSTM(128))\n",
        "  model.add(Dense(256))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "  return model"
      ],
      "metadata": {
        "id": "rQYDMUvJ9Y4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True))\n",
        "\n",
        "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "#model.add(Conv1D(256,5,activation='relu'))\n",
        "model.add(GlobalMaxPool1D())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(len(unique_y), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaaXfzCY9ezM",
        "outputId": "656f073e-350b-4c90-cace-509a675cd3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 32, 100)           100       \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 32, 64)            19264     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 64)            0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 16, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 16, 128)           24704     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 8, 128)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 8, 256)            98560     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 4, 256)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 208677 (815.14 KB)\n",
            "Trainable params: 208677 (815.14 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
      ],
      "metadata": {
        "id": "wnO1qH2m9uzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0LoCMzy90JU",
        "outputId": "345ac7c7-1f2f-4d84-f3ce-5cab1aaca889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to best_model.h5\n",
            "9/9 [==============================] - 3s 163ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 122ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 12/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 13/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 14/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 15/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 16/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 17/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 18/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 18: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 19/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 19: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 20/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 20: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 21/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 21: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 22/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 22: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 23/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 23: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 24: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 121ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 25: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 26/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 26: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 27/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 27: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 28/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 28: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 81ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 29/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 29: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 30/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 30: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 31/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 31: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 32/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 32: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 33/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 33: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 34/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 34: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 35/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 35: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 36/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 36: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 37/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 37: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 38/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 38: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 39/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 39: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 40: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 104ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 41: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 42: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 43/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 43: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 44/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 44: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 45/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 45: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 46/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 46: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 47/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 47: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 48/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 48: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 49/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 49: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 50/50\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.0000e+00\n",
            "Epoch 50: val_loss did not improve from 0.00000\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')"
      ],
      "metadata": {
        "id": "xY7JfdH9-FRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "ind = np.random.randint(0,len(x_val)-1)\n",
        "\n",
        "random_music = x_val[ind]\n",
        "\n",
        "predictions=[]\n",
        "for i in range(10):\n",
        "\n",
        "    random_music = random_music.reshape(1,no_of_timesteps)\n",
        "\n",
        "    prob  = model.predict(random_music)[0]\n",
        "    y_pred= np.argmax(prob,axis=0)\n",
        "    predictions.append(y_pred)\n",
        "\n",
        "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
        "    random_music = random_music[1:]\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqRHN4I5-J6h",
        "outputId": "efd5fad2-8eda-4bdd-e492-4b6a4e8ce057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
        "predicted_notes = [x_int_to_note[i] for i in predictions]"
      ],
      "metadata": {
        "id": "n__i2b3j-UUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import note, chord, stream, instrument\n",
        "\n",
        "def convert_to_midi(prediction_output):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # Convert pattern to string\n",
        "        pattern_str = str(pattern)\n",
        "\n",
        "        # pattern is a chord (if it contains '.' or is a digit)\n",
        "        if ('.' in pattern_str) or pattern_str.isdigit():\n",
        "            notes_in_chord = pattern_str.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                cn = int(current_note)\n",
        "                new_note = note.Note(cn)\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "\n",
        "        # pattern is a single note\n",
        "        else:\n",
        "            new_note = note.Note(pattern_str)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 1\n",
        "\n",
        "    # Create a MIDI stream from the output notes\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    # Write the MIDI file\n",
        "    midi_stream.write('midi', fp='/content/generated_music.mid')"
      ],
      "metadata": {
        "id": "ftIsckd3-WTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_midi(predicted_notes)"
      ],
      "metadata": {
        "id": "5lMC-FkF-lnF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}